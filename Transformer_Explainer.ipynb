{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Explainer\n",
        "\n",
        "The text notes explain the logic prior to key code segments and serve as notes to build intuition about the critical components of the transformer implementation."
      ],
      "metadata": {
        "id": "U4auaIpoQrnz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLome71cI0yK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvjUGX_M1WxA"
      },
      "outputs": [],
      "source": [
        "path = \"gpt/data\"\n",
        "os.makedirs(path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -P gpt/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XcuaRC8Nxaw",
        "outputId": "9c0786e4-6701-46c9-bc50-a1e7f4416992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-10 00:19:58--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘gpt/data/input.txt.4’\n",
            "\n",
            "input.txt.4         100%[===================>]   1.06M  4.82MB/s    in 0.2s    \n",
            "\n",
            "2025-01-10 00:19:58 (4.82 MB/s) - ‘gpt/data/input.txt.4’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = os.path.join(path, 'input.txt')"
      ],
      "metadata": {
        "id": "CAxdmSHhOVFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvp-1fvYgw08"
      },
      "outputs": [],
      "source": [
        "with open(path, \"r\", encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McdPRHVVg6gR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca7c58b-dbed-42a0-e568-a4ef11bf4f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters: 1115394\n"
          ]
        }
      ],
      "source": [
        "print(f\"Length of dataset in characters: {len(text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kPBumt9g-ZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9a7799-8513-4692-a7e8-8fe44aac9e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K3uPryhfQqd2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHkCJZTihHy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25585b18-05be-4a36-9848-0724bc31fc73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Length of vocabulary: 65\n"
          ]
        }
      ],
      "source": [
        "# Get all the unique characters in the dataset.\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(f\"Length of vocabulary: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "A more detailed explaination of the tokenization strategy is covered in the `Tokenizer` repo. This is a quick summary of relevant concepts. \\\n",
        "\\\n",
        "Tokenization is a the process of breaking down the string into components and assign each component a specific integer token. We could assign each word a token, so `\"Hello\"` is `1` and `\"World\"` is `2`. Or we could assign each character a token so `'a'` is `1` and `'b'` is `2`. \\\n",
        "\\\n",
        "Real world implementations of tokenizers use a **\"sub-word\"** tokenization strategy, so common words form a single token but uncommon words are broken up into multiple tokens. \\\n",
        "\\\n",
        "Tokenization strategy determines the size of the vocabulary and the length of the tokenized-string. For instance, for the string `\"Hello world\"`, a word level tokenization has 2 tokens, but for a character level tokenization has 11 tokens. The size of the word level strategy is much larger than that of character level tokenization. \\\n",
        "\\\n",
        "The right tokenization strategy balances between the size of vocabulary and the length of the tokenized input. \\\n",
        "\\\n",
        "In this explainer, to keep things simple, we use a character level tokenizer."
      ],
      "metadata": {
        "id": "evmx81Q7Rz6h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A56YU14thlzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c32931-f397-4157-8943-3012fe096e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2]\n",
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "# Tokenization strategy - map each character to an integer.\n",
        "stoi = { ch:i for i, ch in enumerate(chars)}\n",
        "itos = { i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]  # Take an input string an return the character indices.\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Take a list of character indices and return the string.\n",
        "\n",
        "print(encode(\"Hello, World!\"))\n",
        "print(decode(encode(\"Hello, World!\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Ga0wCxikjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccce6b2c-2751-456d-d7fe-31574bd0f692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# Encode input in torch.Tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])  # Tokens of the first 1000 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpobnCsBjfjX"
      },
      "outputs": [],
      "source": [
        "# Train and validation split.\n",
        "_TRAIN_RATIO = 0.9\n",
        "n = int(_TRAIN_RATIO * len(data))\n",
        "train_data = data[:n]\n",
        "validation_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Length\n",
        "Training the transformer on all of the available data at once is computationally prohibitive. Instead, the input data is randomly sampled for sequences of a fixed maximum length. This is referred to as the **context length** (`block_size` in the code below.) \\\n",
        "\\\n",
        "Each sequence sampled from the input contains multiple training examples. Specifically, a length `N` sequence will contain `N-1` training examples. This is because the token at each position serves as a label for the sequence of tokens preceeding it. So for the token sequence `[1, 2, 3, 4]`:\n",
        "\n",
        "  *   `2` is the label for input `[1]`,\n",
        "  *   `3` is the label for input `[1, 2]` and\n",
        "  *   `4` is the label for input `[1, 2, 3]`.\n",
        "\n",
        "\\\n",
        "Since there are no tokens preceeding the first token (in this case `1`), the first token is not used as a training label. \\\n",
        "\\\n",
        "Besides the computational efficiency, there's another reason why this approach is useful - it also helps to train the transformer of input sequences of varying length (1 to `block_size`.) This is useful in inference when we can use the transformer to generate sequences with an input of as little as 1 token. \\\n",
        "\\\n",
        "In out implementation, `block_size` represents the maximum input size for the transformer. We will sample inputs of length `block_size + 1` so our final example hits the maximum limit for input size.\n",
        "\n"
      ],
      "metadata": {
        "id": "ctOlV8sfc6bP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy3yKL8fpoGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5f831a-049f-4b36-bbb2-6a588f74f22f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "_BLOCK_SIZE = 8\n",
        "train_data[:_BLOCK_SIZE+1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Size\n",
        "As with rest of ML, we batch the data together so different batches can be parallely trained on. Here we batch each sample from the input in its own batch. Out input data now has the following dimensions:\n",
        "*  Batch Size (B): The number of sequences sampled from input text, processed parallelly.\n",
        "*  Block Size (T): The number of tokens in each sampled sequence, to the maximum of `block_size + 1`.\n",
        "*  Examples: Each sampled sequence will contain `block_size` training examples as explained in the Context Length section. \\\n",
        "\\\n",
        "This results in an input tensor of size `batch_size X block_size`. The labels are of the same size (for a samples sequence of length N, inputs are `[0:N-1]` and labels are `[1:N]`.)"
      ],
      "metadata": {
        "id": "X8sfgeQggZU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_vhxsqHGUJh"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "_BATCH_SIZE = 4\n",
        "_BLOCK_SIZE = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  '''Returns a batch of inputs x and target y.'''\n",
        "  data = train_data if split == \"train\" else validation_data\n",
        "  idx = torch.randint(len(data) - _BLOCK_SIZE - 2, (_BATCH_SIZE,))\n",
        "  x = torch.stack([data[i:i+_BLOCK_SIZE] for i in idx])\n",
        "  y = torch.stack([data[i+1:i+_BLOCK_SIZE+1] for i in idx])\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8au7R-2HrrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe47b5b-ac6e-4359-a9de-6ed20ec799db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[56, 41, 46,  1, 53,  5,  1, 58],\n",
            "        [14, 33, 15, 23, 21, 26, 19, 20],\n",
            "        [58, 46, 43, 56, 10,  0, 14, 59],\n",
            "        [47, 60, 43,  1, 51, 43,  1, 58]])\n",
            "Targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[41, 46,  1, 53,  5,  1, 58, 46],\n",
            "        [33, 15, 23, 21, 26, 19, 20, 13],\n",
            "        [46, 43, 56, 10,  0, 14, 59, 58],\n",
            "        [60, 43,  1, 51, 43,  1, 58, 46]])\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch(\"train\")\n",
        "print('Inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('Targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Table\n",
        "Embedding tables are a common approach in NLP (and other ML fields) to reduce dimensionality by mapping higer dimensional spaces to lower dimensional spaces. *Dimension* here just means the number of elements in the feature vector. \\\n",
        "\\\n",
        "As instance, if we were to use one-hot encoding to represent a movie in the entire universe of movies, the length of the feature vector would be in millions. However, we could represent each movie as a 3-dimensional vector of [`runtime_in_minutes`, `imdb_rating`, `genre`], which would be a much more feasible model. \\\n",
        "\\\n",
        "Refer to [this ~2-min video by Google](https://www.youtube.com/watch?v=my5wFNQpFO0&t=1s) for a more detailed explaination on embeddings. \\\n",
        "\\\n",
        "In our example, each token is represented with an embedding table of size `vocab_size`. While this is the same dimension as one-hot encoding the tokens, this allows us to use the embedding as a probability function - the value at each position will indicate the probability of the corresponding token to be appear next. In the future sections when we don't use the output of the embedding table as the final distribution, this size will change.\n",
        "\n",
        "### Dimensions\n",
        "With each token being represented by its embedding, we've added a new dimension to our data:\n",
        "*  Channels (C): The `vocab_size` representation of the token.\n"
      ],
      "metadata": {
        "id": "J9fySIFRitdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GDU1KNZ2oCRT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WznC43uH5Av"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Next token prediction for token idx.\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # Compute loss - negative log liklihood.\n",
        "      # Expected loss without training - -ln(1/65) since liklihood of correct\n",
        "      # prediction is 1/65 without training.\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"Generate the next token given the context idx.\n",
        "\n",
        "    For the BigramModel, this generation only needs the last character, but this\n",
        "    function uses the entire history - i.e. all the concatenated inputs. This\n",
        "    will be kept constant and used in GPT when we use all the history.\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # Get predictions.\n",
        "      logits, loss = self(idx)  # Defaults to self.forward(). Make targets optional in forward().\n",
        "\n",
        "      # Get the output of the last time-step.\n",
        "      logits = logits[:, -1, :]  # becomes (B, C)\n",
        "\n",
        "      # Softmax to get probabilities.\n",
        "      probs = F.softmax(logits, dim=-1) # Still (B, C)\n",
        "\n",
        "      # Generate a sample from the distribution of tokens.\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "      # Append prediction to index.\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample\n",
        "We can use the untrained model created above to generate some text and compute the Cross-Entropy loss. Since it's not been trained yet, we expect it to produce garbage data and can also anticipate the error (based on random prediction over 65 possibilities.)\n"
      ],
      "metadata": {
        "id": "PsW6vr8ap2Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "generated_text = decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "print(f\"Generated Text:{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KwddyrzqU60",
        "outputId": "dc39482c-45bf-42f3-9b1f-21c0e22aecdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.404358386993408\n",
            "Generated Text:\n",
            "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
            "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D0EUwfkYh3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed9fe0f-0e10-43c2-a65d-20c1a44c43bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.581765174865723\n",
            "Loss: 3.638050079345703\n",
            "Loss: 3.1831507682800293\n",
            "Loss: 2.703016996383667\n",
            "Loss: 2.612287759780884\n",
            "Loss: 2.675128221511841\n",
            "Loss: 2.5464842319488525\n",
            "Loss: 2.5170984268188477\n",
            "Loss: 2.445920944213867\n",
            "Loss: 2.4285647869110107\n"
          ]
        }
      ],
      "source": [
        "# Create an optimizer.\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "_BATCH_SIZE = 32\n",
        "for steps in range(10000):\n",
        "  # Sample a batch of data.\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  # Evaluate the loss.\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if steps % 1000 == 0:\n",
        "    print(f\"Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample\n",
        "Here's the loss and a sample of the generated text after training. \\\n",
        "\\\n",
        "**NOTE:** Here we're only using a Bigram model, which is only looking at character pairs. So while the input is the entire context length, only the last token is used to predict the next token."
      ],
      "metadata": {
        "id": "6leRYcySr1HA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RgJciLyY7RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b5310a-4a0a-47ec-9fa7-7e153eaf543a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.455496072769165\n",
            "Generated Text: \n",
            "Ong h hasbe pave pirance\n",
            "RDe hicomyonthar's\n",
            "PES:\n",
            "AKEd ith henourzincenonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KINld pe wither vouplloutherccnohathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so itJas\n",
            "Waketancotha:\n",
            "h haybet--s n prids, r loncave w hollular s O:\n",
            "HIs; ht anjx?\n",
            "\n",
            "DUThinqunt.\n",
            "\n",
            "LaZEESTEORDY:\n",
            "h l.\n",
            "KEONGBUCHandspo be y,-JZNEEYowddy scar t tridesar, wyonthenous s ls, theresseys\n",
            "PlorseelapinghienHen yof GLUCEN t l-h!E:\n",
            "I hisgothers je are!-e!\n",
            "QUCotouciullle's fld\n"
          ]
        }
      ],
      "source": [
        "generated_text = decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhqfqQwgRU3K"
      },
      "source": [
        "## Beyond Bigram\n",
        "Until this point, we've been looking only at the current token to predict the next one and we've all of the remaining tokens in the context length. We now want to use **all** of the tokens in our context to predict the next token. \\\n",
        "*  First, we need to ensure that for each prediction, we're only using tokens that appear *before* the target token in the context. We shouldn't be able to look at the future tokens because we're trying to predict those.\n",
        "*  Second, we need a way to aggregate the information from each of the prior tokens together. For now, we'll use average of all of the embedding vectors preceeding the target token. This is not ideal but good for illustration. \\\n",
        "\n",
        "\n",
        "We've seen how the current dimension for our input is `B x T x C`. Now, for each batch, the token at position `t` will now be an average of off all the embedding vectors of all tokens *preceeding* `t`. This will generate an input of dimension `t x C` for each position `t`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciJK_EGAZsaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd0df1e-c072-41f4-c5ce-32e026969e20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "from typing import cast\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 2  # Batch, Time and Channels.\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts0Y0C1GTb6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5896a001-e0a8-4427-df01-fb1ad98abf41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x[0]:\n",
            "\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ],
      "source": [
        "# If we want x[b, t] = mean_{i<=t} x[b, i]\n",
        "\n",
        "xbow = torch.zeros((B, T, C))  # Bag of words since we're averaging everything.\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1]  # (t, C)\n",
        "    xbow[b, t] = torch.mean(xprev, 0)\n",
        "\n",
        "print(\"x[0]:\\n\")\n",
        "print(x[0])\n",
        "print(xbow[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization\n",
        "In the code snippet above, we went through the aggregation process using two nested loops. This is terribly inefficeint. If we want to compute the dot-product of two lists of numbers, we could do a similar nested loop strategy or we could convert the lists into PyTorch 1-D tensors and use the PyTorch vector operations. Linear Algebra operations in frameworks like PyTorch and TensorFlow and highly optimized operations making them significantly more compute-effecient that nested loops. So, as much as possible, we want to write our implementation in terms of linear algebra operations as possible. This process is called vectorization. In practice, several of the innefficiencies in training implementations stem from operations that aren't correctly vectorized. \\\n",
        "\\\n",
        "To vectorize the above implementation\n",
        "*  We multiply the inputs with a lower-triangular matrix of `1's`. This ensures that for each position, we only consider the preceeding tokens.\n",
        "*  We then compute the average at each position, but dividing by the vector sum.\n",
        "*  The above two operations can be merged - instead of a lower triangular matrix of `1's`, we can use a lower-triangular matrix of fractions such that each row adds up to `1`. In the code below, this is represented by the `weights` matrix."
      ],
      "metadata": {
        "id": "FvrC6O7RdCyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8JgcPOrVxgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166f69d3-664f-4169-dd27-8a9a78767fa4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "weights = torch.tril(torch.ones(T, T))\n",
        "weights = weights / weights.sum(1, keepdim=True)\n",
        "xbow2 = weights @ x  # Batched multiplication. (B, T, T) x (B, T, C) => Each batch will have (T, T) x (T, C) product -> (B, T, C)\n",
        "xbow2[0]\n",
        "torch.allclose(xbow2, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Static Weights\n",
        "Vectorizing solved the problem of inefficient computation of weights for the prior tokens, however the final weights are still static. In the transformer, we want the token at each position to *learn* the weights for the preceeding tokens. That way individual tokens can have different weights when aggregating the past tokens. To do this, we use a different implementation to create the same mask - using the **Softmax** function. \\\n",
        "\\\n",
        "Each token is assigned a 0-weight and the tokens that appear *after* the target token are assigned `-inf`. We then take a Softmax over the row elements ($\\frac{e^i}{\\sum_{i}e^i}$), where the `-inf` elements become 0 ($\\frac{1}{e^∞} = 0$) and the remaining are normalized ($\\frac{1}{e^0} = 0$). This forms the framework for learnt weights in the future."
      ],
      "metadata": {
        "id": "tLC9I8LstRXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ulEfHnYBw6nG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FKj3Nz3XZwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510323cb-f63f-4898-ac27-7fbca43ff0c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "# 2nd implementation - use Softmax.\n",
        "# Softmax also has a normalizing operation - -inf becomes zero and same items\n",
        "# have same probabilities.\n",
        "# Softmax is preferred since now we can adjust the weights of previous tokens so\n",
        "# that the weights are now\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "weights = torch.zeros((T, T))\n",
        "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
        "weights = F.softmax(weights, dim=-1)\n",
        "weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Building the Transformer\n",
        "### Adding a head\n",
        "In our current approach, we've taken the token index to retrieve a corresponding embedding. We've used the embedding dimension of the size of vocabulary so the embedding could encode a probability distribution.\\\n",
        "\\\n",
        "We're now adding another layer to our model - a linear layer that we're calling a *head*. We're now using the output of this head to encode the probabilities. The inputs to the head will be the token embeddings we retrieve from the embedding table. Since the embedding is no longer used as a probability distribution, we can now reduce the dimensions of the embedding, as hinted in the embeddings section before.\n",
        "\n",
        "### Positional Embeddings\n",
        "With aggregating the preceeding tokens, we're able to look at the entire context length available to the transformer. However, notice how the embedding for each token is only dependant on the token value. The relative positions of the preceeding tokens is not yet encoded. \\\n",
        "\\\n",
        "To change this, we adding positional encodings to our token embeddings. Each token embedding is a value from `0` to `block_size - 1` and will have an embedding of the same dimension as the token embedding."
      ],
      "metadata": {
        "id": "-RQedA9gfdA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 32\n",
        "block_size = _BLOCK_SIZE"
      ],
      "metadata": {
        "id": "7aRQDrkVK035"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Token Embedding.\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    # Positional Embedding.\n",
        "    self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # Output Head.\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # Next token prediction for token idx.\n",
        "    token_embedding = self.token_embedding_table(idx)  # B, T, C.\n",
        "    pos_embedding = self.positional_embedding_table(torch.arange(T))  # T, C\n",
        "    x = token_embedding + pos_embedding # (B, T, C)\n",
        "    logits = self.lm_head(token_embedding) # B, T, vocab_size\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"Generate the next token given the context idx.\"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # Crop Idx to the last block_size tokens since position embedding will\n",
        "      # only contains indices upto block size.\n",
        "      idx_crop = idx[:, -block_size:]\n",
        "      # Get predictions.\n",
        "      logits, loss = self(idx_crop)  # Defaults to self.forward(). Make targets optional in forward().\n",
        "\n",
        "      # Get the output of the last time-step.\n",
        "      logits = logits[:, -1, :]  # becomes (B, C)\n",
        "\n",
        "      # Softmax to get probabilities.\n",
        "      probs = F.softmax(logits, dim=-1) # Still (B, C)\n",
        "\n",
        "      # Generate a sample from the distribution of tokens.\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "      # Append prediction to index.\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "dIOuyZbDHeo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x2, y2 = get_batch(\"train\")\n",
        "m2 = LanguageModel()\n",
        "logits, loss = m2(x2, y2)\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "decode(m2.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xDbCW5QnKT7h",
        "outputId": "5697d40c-aaa2-42ff-c679-731f992d7327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.323673248291016\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nGqTIFOtZYh!nNNB33BqJsGsF?-xAIpfcePfyjhYGv.pl'oevfBNZ. qYvj'Y?eC,pVbPM$U,ISeX'ApBXU?z?w'wF\\n!fv$3Ztw,H\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_BATCH_SIZE = 32\n",
        "optimizer = torch.optim.AdamW(m2.parameters(), lr=1e-3)\n",
        "for steps in range(10000):\n",
        "  # Sample a batch of data.\n",
        "  x2, y2 = get_batch(\"train\")\n",
        "\n",
        "  # Evaluate the loss.\n",
        "  logits, loss = m2(x2, y2)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if steps % 1000 == 0:\n",
        "    print(f\"Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMfS9dreKgLD",
        "outputId": "c6a2a589-d5ab-4e56-f791-d486ffacbd4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.242744445800781\n",
            "Loss: 2.469791889190674\n",
            "Loss: 2.4862873554229736\n",
            "Loss: 2.465446949005127\n",
            "Loss: 2.400810718536377\n",
            "Loss: 2.5412375926971436\n",
            "Loss: 2.5605099201202393\n",
            "Loss: 2.331833600997925\n",
            "Loss: 2.573298454284668\n",
            "Loss: 2.5379536151885986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2, y2 = get_batch(\"train\")\n",
        "logits, loss = m2(x2, y2)\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "decode(m2.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "uNseR-ZkkaTU",
        "outputId": "48854137-1c8f-4312-8d3b-8a02fff22c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.4701757431030273\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLOULI f iobettesotrits, the sthy tem ardors hthande che thel shithelthave ureest s, stem INLEENRO:\\nS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFZFN5WYjypX"
      },
      "source": [
        "# Self-Attention intuition\n",
        "Now that we have positional embeddings for our tokens, we want to combine the tokens in such a way that any previous tokens that are more relevant to the current token are emphasized and those that aren't relevant are penalized.\n",
        "\n",
        "## Solution with self-attention\n",
        "In self-attention, the token at each position will generate two embeddings - the key and the query. Intuitively the key is an embedding representation of what this token at this position contains. The query is the embedding representing 'What would be the most relevant information for this token?'\n",
        "\n",
        "Then, the dot product between the query embedding of the current position, with the key token of all previous tokens will result in positive correlation if the relevance is high, and will be low (or negative) otherwise. These dot-products will be the weights that we want to use when averaging the previous token embeddings.\n",
        "\n",
        "## Intuitve role of Q, K and V.\n",
        "For any token, `X` is the information private to the token. That is what the token _really_ is. `Q` is a representation of what this token is looking for, based on data. `K` is a representation of the token's response to queries by other tokens (For instance, I'm a noun.) `V` is what this token will provide other tokens if their `Q.K` have high affinity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nROD5i_dVNBd"
      },
      "outputs": [],
      "source": [
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Single head with self-attention.\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)  # Mat-Mul with fixed weights.\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "# Value vector is the one that actually represents the token and this is what we\n",
        "# aggregate when computing the embedding for the current token.\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "# For each batch in B, the (T, C) matrix gets multiplied by a (C, head_size)\n",
        "# mat-mul layer producing a (B, T, head_size) result.\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "\n",
        "\n",
        "\n",
        "# Transpose the last (head_size) and second last (T), leaving Batch untouched.\n",
        "# keys are now (B, head_size, T) and product will be (B, T, T) - each position has a\n",
        "# weight vector over all other positions.\n",
        "weights = q @ k.transpose(-2, -1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
        "weights = F.softmax(weights, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "output = weights @ v"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\"Single self-attention head.\"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)  # Setting bias false means this is basically a matrix multiply operation.\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(_BLOCK_SIZE, _BLOCK_SIZE)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)    # (B, T, C)\n",
        "    q = self.query(x)  # (B, T, C)\n",
        "    # Compute attention scores (\"affinities\")\n",
        "    weights = q @ k.transpose(-2, -1) * C**-0.5  # Normarlized by channel size to normalize embeddings.\n",
        "    weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "    weights = F.softmax(weights, dim=-1)  # (B, T, T)\n",
        "    # Perform the weighted aggregation of the values.\n",
        "    v = self.value(x)  # (B, T, C)\n",
        "    out = weights @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "3RKfBzfW2c8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Token Embedding.\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    # Positional Embedding.\n",
        "    self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # Self Attention Head\n",
        "    self.sa_head = Head(n_embd)\n",
        "    # Output Head.\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # Next token prediction for token idx.\n",
        "    token_embedding = self.token_embedding_table(idx)  # B, T, C.\n",
        "    pos_embedding = self.positional_embedding_table(torch.arange(T))  # T, C\n",
        "    x = token_embedding + pos_embedding # (B, T, C)\n",
        "    x = self.sa_head(x)  # Apply self attention.\n",
        "    logits = self.lm_head(token_embedding) # B, T, vocab_size\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"Generate the next token given the context idx.\"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # Crop Idx to the last block_size tokens since position embedding will\n",
        "      # only contains indices upto block size.\n",
        "      idx_crop = idx[:, -block_size:]\n",
        "      # Get predictions.\n",
        "      logits, loss = self(idx_crop)  # Defaults to self.forward(). Make targets optional in forward().\n",
        "\n",
        "      # Get the output of the last time-step.\n",
        "      logits = logits[:, -1, :]  # becomes (B, C)\n",
        "\n",
        "      # Softmax to get probabilities.\n",
        "      probs = F.softmax(logits, dim=-1) # Still (B, C)\n",
        "\n",
        "      # Generate a sample from the distribution of tokens.\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "      # Append prediction to index.\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "BNPujK2B4LrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x3, y3 = get_batch(\"train\")\n",
        "m3 = SelfAttentionLanguageModel()\n",
        "logits, loss = m3(x3, y3)\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "decode(m3.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wru14cD94w7F",
        "outputId": "ec94c867-a284-4b8f-fc2e-8dcb8ecaa72b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.424878120422363\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\neDQ3QAKdPobjyHOG$MP;vO-uJTnOKhjxqaaRz.ypk3OFbY'LjDk3&QuN:qR.NkfIODK-KWz$q&IC?BysmjXxBnxQRdLKhEM ?U?-\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_BATCH_SIZE = 32\n",
        "optimizer = torch.optim.AdamW(m3.parameters(), lr=1e-3)\n",
        "for steps in range(10000):\n",
        "  # Sample a batch of data.\n",
        "  x3, y3 = get_batch(\"train\")\n",
        "\n",
        "  # Evaluate the loss.\n",
        "  logits, loss = m3(x3, y3)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if steps % 1000 == 0:\n",
        "    print(f\"Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnZ5HsHp4qQY",
        "outputId": "ac633e33-6478-440c-9b61-f0271c486427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.418106555938721\n",
            "Loss: 2.7019495964050293\n",
            "Loss: 2.554936647415161\n",
            "Loss: 2.6280925273895264\n",
            "Loss: 2.4184787273406982\n",
            "Loss: 2.428046464920044\n",
            "Loss: 2.3833444118499756\n",
            "Loss: 2.4863102436065674\n",
            "Loss: 2.427128791809082\n",
            "Loss: 2.4417405128479004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x3, y3 = get_batch(\"train\")\n",
        "logits, loss = m3(x3, y3)\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "print(\n",
        "    decode(m3.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist())\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssi80IYQ5SMB",
        "outputId": "c5218876-c9a1-48c5-e0e4-cd29def928bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.4954659938812256\n",
            "\n",
            "CI cy agist hy hatas,'s 'k touther\n",
            "Th,\n",
            "Varo sprer de I ighiody richip spre!\n",
            "S:\n",
            "Whateld weth I it ing tl Talouthaitid t alyop I cos winourngond pld he ang'larenncord amer, Go k'sthe r\n",
            "Thesele; wh st, yotinoowind h t Wiowe sose we h fund y,\n",
            "Wilsthent;\n",
            "\n",
            "BRor:\n",
            "E:\n",
            "I:\n",
            "O Byay hal rstye tound d teaie,sh urewimatoornen alfrdthands, d wr t mou is pin at wames ngey th, win.\n",
            "Jun my f has ifur.\n",
            "OFo wanghele hath t lldoww II o hiedes?\n",
            "\n",
            "You s, hinton d, bu whndes d ks\n",
            "Thig y t I asisemowakealoor.\n",
            "\n",
            "Yove roy s befathowentid timenousthis pu is ber,\n",
            "I mer ker'de:\n",
            "Bee CHUSHe, omesenyorrif ULADWI mo l d il ik; ash, kn h an phengnglosothin spullthitcks kn, rde nde.\n",
            "I g irerllle sioutave anchanind ar withe d othat n m avecaghrle hind am ld whug GUS:\n",
            "Fowneatoraiy omolodeack beronon he butous breitothablt,\n",
            "O:\n",
            "aty or d fe sengund o f f t, Vareaye.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Asosirf h's:\n",
            "CULIO:\n",
            "Thonothene tlofu ouprolldund'den ffolth. cher\n",
            "CEvempr wansatifoundethelleency hise, EE:\n",
            "\n",
            "ICEEOLLI igoved;\n",
            "\n",
            "\n",
            "Ha whin s lBur thn. hr, so iocy th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a16kvNkyqtLu"
      },
      "source": [
        "# Self-Attention abstractly\n",
        "\n",
        "\n",
        "*   Self-attention is a communication mechanism between a nodes in a directed graph. It so happens that in a language model, the nodes are structured in a way that current node has edges coming from all previous nodes (edges indicate direction of information flow.)\n",
        "*   There is no notion of space in attention modeling, there are just vectors that determine affinity and weighting of values. Which is why positional embedding is additionally added.\n",
        "\n",
        "*   There is no communication between the tokens across batches\n",
        "*   In some language models (eg BERT) all the nodes are allowed to talk to each other and directionality is not limited. The will have an \"encoder\" block that doesn't have the lower-triangular masking. Current implementation is a decoder block to have the lower-triangular structure.\n",
        "*   Self-Attention - Q, K and V come from the same token. There can be other attention mechanisims where the queries are produced from X, but K and V come from the encoder block which may add additional context. Cross-Attention - when there is separate set of nodes where we get the information from.\n",
        "*   In the paper, the aggregation of values get scaled by a factor of sqrt(head_size). This is done for the following reason: Q and K are unit gaussian vectors. If Q.K is performed without normalization, variance of the product is in the order of head_size. If the variance of the inputs to the softmax is too strong, the softmax will become more \"sharp\" and start getting closer to a one-hot-encoding. This will result in each node only taking inputs from one other node. Normalizing with sqrt(head_size) fixes this issue.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoTGkM6Nn-d6"
      },
      "outputs": [],
      "source": [
        "# Weights is no longer a constant, but unique to each batch because each batch\n",
        "# now different tokens and weigh is token dependent.\n",
        "weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "*  Andrej Karpathy's [GPT-2 Tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
        "*  [Attention is All You Need paper](https://arxiv.org/abs/1706.03762)\n",
        "*  [OpenAI GPT-3 paper](https://arxiv.org/abs/2005.14165)\n"
      ],
      "metadata": {
        "id": "7No0pnkY3jTZ"
      }
    }
  ]
}